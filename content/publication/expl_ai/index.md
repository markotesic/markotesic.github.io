---
authors:
  - admin
  - Ulrike Hahn
# author_notes:
# - "Equal contribution"
# - "Equal contribution"
publication_short: ""
abstract: "In this chapter, we consider recent work aimed at guiding the design of algorithmically generated explanations. The chapter proceeds in four parts. Firstly, we introduce the general problem of machine-generated explanation and illustrate different notions of explanation with the help of Bayesian belief networks. Secondly, we introduce key theoretical perspectives on what constitutes an explanation, and more specifically a 'good' explanation, from the philosophy literature. We compare these theoretical perspectives and the criteria they propose with a case study on explaining reasoning in Bayesian belief networks and present implications for AI. Thirdly, we consider the pragmatic nature of explanation with the focus on its communicative aspects that are manifested in considerations of trust. Finally, we present conclusions."
tags: []
projects: []
slides: ""
url_pdf: ""
publication_types:
  - "1"
image:
  caption: ""
  focal_point: ""
  preview_only: false
  filename: featured.jpg
summary: "What do we do with our existing models when we encounter new variables to consider? Does the order in which we learn variables matter? The paper investigates two modeling strategies and experimentally tests how people reason when presented with new variables and in different orders."
url_dataset: ""
url_project: ""
url_source: ""
url_video: ""
publication: "*Human-Like Machine Intelligence*"
featured: true
date: 2021-01-01
url_slides: ""
title: "Explanation in AI systems"
url_poster: ""
url_code: ""
doi: ""
---
