---
authors:
  - admin
  - Janet Slesinski 
  
#author_notes:
#  - "Equal contribution"
#  - "Equal contribution"
publication_short: ""
abstract: "As AI models increasingly tackle complex tasks, evaluating their mathematical reasoning capabilities has become essential. However, designing effective benchmarks that accurately assess a model's reasoning abilities in mathematics requires careful consideration of various parameters. This paper outlines key aspects in developing robust benchmarks for evaluating large language models (LLMs) in mathematical reasoning, highlights limitations of existing assessments, and proposes criteria for comprehensive evaluations."
tags: []
projects: []
slides: ""
url_pdf: "https://curriculumredesign.org/wp-content/uploads/Benchmark-design-criteria-for-mathematical-reasoning-in-LLMs.pdf"

#links:
#  - icon: twitter
#    icon_pack: fab
#   - name: Website
#     url: https://ai-evaluation-paradigms.github.io
#   - name: GitHub
#     url: https://github.com/Kinds-of-Intelligence-CFI/Paradigms-of-AI-Evaluation

publication_types:
  - "1"
image:
  caption: ""
  focal_point: ""
  preview_only: false
  filename: #featured.png
summary: "I lay out key benchmark design criteria for evaluating mathematical reasoning in LLMs."
url_dataset: ""
url_project: ""
url_source: ""
url_video: ""
publication: "*Center for Curriculum Redesign*"
featured: false
date: 2025-01-03
url_slides: ""
title: "Benchmark Design Criteria for Mathematical Reasoning in LLMs"
url_poster: ""
url_code: ""
doi: ""
---
